{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minist Basic Approach- (Softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_DATA/',one_hot= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24c4f25cda0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_image = mnist.train.images[1].reshape(28,28)\n",
    "plt.imshow(single_image,cmap = 'gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Place Holders\n",
    "x = tf.placeholder(tf.float32,shape=[None,784])\n",
    "y_true = tf.placeholder(tf.float32,[None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Variable ops\n",
    "w = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph Ops\n",
    "y = tf.matmul(x,w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Loss function (softmax cross entropy loss function)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_true,logits=y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optimizer and train function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9193\n"
     ]
    }
   ],
   "source": [
    "# Create Session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        batch_x,batch_y = mnist.train.next_batch(100)\n",
    "        sess.run(train,feed_dict={x:batch_x,y_true:batch_y})\n",
    "    \n",
    "    # Evaluate the model\n",
    "    pred = tf.equal(tf.argmax(y,1),tf.argmax(y_true,1))\n",
    "    \n",
    "    # convert[True,False,True,...] to [1,0,1,...]\n",
    "    acc = tf.reduce_mean(tf.cast(pred,tf.float32))\n",
    "    print(sess.run(acc,feed_dict={x:mnist.test.images, y_true:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Theory\n",
    "\n",
    "Convolution Neural Network has a major advantage for image processing, where pixels nearby to each other are much more correlated to each other for image Detection.\n",
    "\n",
    "Each CNN layer looks at an increasingly larger part of the image\n",
    "Having units only connected to nearby units also aids in invariance.\n",
    "CNN also helps with regularization, limiting the search of weights to the size of the convolution\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minist CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_DATA/',one_hot= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"HELPER FUNCTIONS\"\"\"\n",
    "\n",
    "# Init Weights\n",
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape,stddev = 0.1)\n",
    "    return tf.Variable(init_random_dist)\n",
    "\n",
    "# Init Bias\n",
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)\n",
    "\n",
    "# Convolution 2D\n",
    "def conv2d(x,w):\n",
    "    \"\"\" x --> [batch, Height,Weight,Channels]\"\"\"\n",
    "    \"\"\" w --> [filter Height, filter Weight, Channels IN, Channels OUT]\"\"\"\n",
    "    return tf.nn.conv2d(x,w,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "# Pooling\n",
    "def max_pool_2by2(x):\n",
    "    # x-->[batch, h, w, c]\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer\n",
    "def convolutional_layer(input_x,shape):\n",
    "    w = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x,w)+b)\n",
    "\n",
    "# Normal Layer (Full Connected)\n",
    "def normal_full_layer(input_layer,size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    w = init_weights([input_size,size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer,w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnStep: 0\n",
      "Accuracy: \n",
      "0.1254\n",
      "\n",
      "\n",
      "OnStep: 100\n",
      "Accuracy: \n",
      "0.9428\n",
      "\n",
      "\n",
      "OnStep: 200\n",
      "Accuracy: \n",
      "0.9601\n",
      "\n",
      "\n",
      "OnStep: 300\n",
      "Accuracy: \n",
      "0.9656\n",
      "\n",
      "\n",
      "OnStep: 400\n",
      "Accuracy: \n",
      "0.9734\n",
      "\n",
      "\n",
      "OnStep: 500\n",
      "Accuracy: \n",
      "0.9757\n",
      "\n",
      "\n",
      "OnStep: 600\n",
      "Accuracy: \n",
      "0.9796\n",
      "\n",
      "\n",
      "OnStep: 700\n",
      "Accuracy: \n",
      "0.9818\n",
      "\n",
      "\n",
      "OnStep: 800\n",
      "Accuracy: \n",
      "0.9846\n",
      "\n",
      "\n",
      "OnStep: 900\n",
      "Accuracy: \n",
      "0.9851\n",
      "\n",
      "\n",
      "OnStep: 1000\n",
      "Accuracy: \n",
      "0.9788\n",
      "\n",
      "\n",
      "OnStep: 1100\n",
      "Accuracy: \n",
      "0.9843\n",
      "\n",
      "\n",
      "OnStep: 1200\n",
      "Accuracy: \n",
      "0.9861\n",
      "\n",
      "\n",
      "OnStep: 1300\n",
      "Accuracy: \n",
      "0.9865\n",
      "\n",
      "\n",
      "OnStep: 1400\n",
      "Accuracy: \n",
      "0.9879\n",
      "\n",
      "\n",
      "OnStep: 1500\n",
      "Accuracy: \n",
      "0.9871\n",
      "\n",
      "\n",
      "OnStep: 1600\n",
      "Accuracy: \n",
      "0.9855\n",
      "\n",
      "\n",
      "OnStep: 1700\n",
      "Accuracy: \n",
      "0.9856\n",
      "\n",
      "\n",
      "OnStep: 1800\n",
      "Accuracy: \n",
      "0.985\n",
      "\n",
      "\n",
      "OnStep: 1900\n",
      "Accuracy: \n",
      "0.9903\n",
      "\n",
      "\n",
      "OnStep: 2000\n",
      "Accuracy: \n",
      "0.9874\n",
      "\n",
      "\n",
      "OnStep: 2100\n",
      "Accuracy: \n",
      "0.9872\n",
      "\n",
      "\n",
      "OnStep: 2200\n",
      "Accuracy: \n",
      "0.9897\n",
      "\n",
      "\n",
      "OnStep: 2300\n",
      "Accuracy: \n",
      "0.987\n",
      "\n",
      "\n",
      "OnStep: 2400\n",
      "Accuracy: \n",
      "0.9901\n",
      "\n",
      "\n",
      "OnStep: 2500\n",
      "Accuracy: \n",
      "0.9874\n",
      "\n",
      "\n",
      "OnStep: 2600\n",
      "Accuracy: \n",
      "0.9879\n",
      "\n",
      "\n",
      "OnStep: 2700\n",
      "Accuracy: \n",
      "0.9877\n",
      "\n",
      "\n",
      "OnStep: 2800\n",
      "Accuracy: \n",
      "0.9903\n",
      "\n",
      "\n",
      "OnStep: 2900\n",
      "Accuracy: \n",
      "0.9892\n",
      "\n",
      "\n",
      "OnStep: 3000\n",
      "Accuracy: \n",
      "0.9881\n",
      "\n",
      "\n",
      "OnStep: 3100\n",
      "Accuracy: \n",
      "0.9891\n",
      "\n",
      "\n",
      "OnStep: 3200\n",
      "Accuracy: \n",
      "0.9887\n",
      "\n",
      "\n",
      "OnStep: 3300\n",
      "Accuracy: \n",
      "0.9882\n",
      "\n",
      "\n",
      "OnStep: 3400\n",
      "Accuracy: \n",
      "0.9907\n",
      "\n",
      "\n",
      "OnStep: 3500\n",
      "Accuracy: \n",
      "0.9906\n",
      "\n",
      "\n",
      "OnStep: 3600\n",
      "Accuracy: \n",
      "0.9897\n",
      "\n",
      "\n",
      "OnStep: 3700\n",
      "Accuracy: \n",
      "0.9878\n",
      "\n",
      "\n",
      "OnStep: 3800\n",
      "Accuracy: \n",
      "0.9902\n",
      "\n",
      "\n",
      "OnStep: 3900\n",
      "Accuracy: \n",
      "0.9914\n",
      "\n",
      "\n",
      "OnStep: 4000\n",
      "Accuracy: \n",
      "0.9907\n",
      "\n",
      "\n",
      "OnStep: 4100\n",
      "Accuracy: \n",
      "0.9903\n",
      "\n",
      "\n",
      "OnStep: 4200\n",
      "Accuracy: \n",
      "0.9899\n",
      "\n",
      "\n",
      "OnStep: 4300\n",
      "Accuracy: \n",
      "0.9902\n",
      "\n",
      "\n",
      "OnStep: 4400\n",
      "Accuracy: \n",
      "0.9897\n",
      "\n",
      "\n",
      "OnStep: 4500\n",
      "Accuracy: \n",
      "0.9918\n",
      "\n",
      "\n",
      "OnStep: 4600\n",
      "Accuracy: \n",
      "0.9919\n",
      "\n",
      "\n",
      "OnStep: 4700\n",
      "Accuracy: \n",
      "0.9906\n",
      "\n",
      "\n",
      "OnStep: 4800\n",
      "Accuracy: \n",
      "0.9901\n",
      "\n",
      "\n",
      "OnStep: 4900\n",
      "Accuracy: \n",
      "0.9878\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32,shape=[None,784])\n",
    "y_true = tf.placeholder(tf.float32, shape = [None,10])\n",
    "\n",
    "# Layers\n",
    "x_image = tf.reshape(x,[-1,28,28,1])\n",
    "conv_1 = convolutional_layer(x_image,shape=[5,5,1,32])\n",
    "conv_1_pooling = max_pool_2by2(conv_1)\n",
    "conv_2 = convolutional_layer(conv_1_pooling,shape=[5,5,32,64])\n",
    "conv_2_pooling = max_pool_2by2(conv_2)\n",
    "conv_2_flat = tf.reshape(conv_2_pooling,[-1,7*7*64])\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(conv_2_flat,1024))\n",
    "\n",
    "# Dropout\n",
    "hold_prob = tf.placeholder(tf.float32)\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)\n",
    "\n",
    "# Predict\n",
    "y_pred = normal_full_layer(full_one_dropout,10)\n",
    "\n",
    "# Loss Function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y_true, logits=y_pred))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train=optimizer.minimize(loss)\n",
    "\n",
    "# Session\n",
    "steps = 5000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(steps):\n",
    "        batch_x,batch_y = mnist.train.next_batch(50)\n",
    "        sess.run(train, feed_dict={x:batch_x,y_true:batch_y,hold_prob:0.5})\n",
    "        \n",
    "        if i%100 ==0:\n",
    "            print(\"OnStep: {}\".format(i))\n",
    "            print(\"Accuracy: \")\n",
    "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
    "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "            print(sess.run(acc,feed_dict={x:mnist.test.images,y_true:mnist.test.labels,hold_prob:1.0}))\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
