{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow', '.', 'Tomorrow', 'is', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Text\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "string = \"The science of today is the technology of tomorrow. \\\n",
    "    Tomorrow is today.\"\n",
    "\n",
    "# Tokenize words\n",
    "tokenized_words = word_tokenize(string)\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentences\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/suzuka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "# Load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Show stop words\n",
    "stops = stop_words[:]\n",
    "print(len(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'science', 'today', 'technology', 'tomorrow', '.', 'Tomorrow', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "nonstop_words = []\n",
    "for word in tokenized_words:\n",
    "    if word not in stop_words:\n",
    "        nonstop_words.append(word)\n",
    "print(nonstop_words)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "scienc\n",
      "of\n",
      "today\n",
      "is\n",
      "the\n",
      "technolog\n",
      "of\n",
      "tomorrow\n",
      ".\n",
      "tomorrow\n",
      "is\n",
      "today\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Apply stemmer\n",
    "for word in tokenized_words:\n",
    "    print(porter.stem(word))\n",
    "    \n",
    "#[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('science', 'NN'), ('of', 'IN'), ('today', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('technology', 'NN'), ('of', 'IN'), ('tomorrow', 'NN'), ('.', '.'), ('Tomorrow', 'NN'), ('is', 'VBZ'), ('today', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Tag Parts Of Speech\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Use pre-trained part of speech tagger\n",
    "text_tagged = pos_tag(word_tokenize(string))\n",
    "print(text_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words model\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Create text\n",
    "text_data = np.array(['I love San Francisco!',\n",
    "                      'New York is best',\n",
    "                      'LA beats both'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the bag of words feature matrix\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "# Show feature matrix\n",
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beats', 'best', 'both', 'francisco', 'is', 'la', 'love', 'new', 'san', 'york']\n"
     ]
    }
   ],
   "source": [
    "# Get feature names\n",
    "feature_names = count.get_feature_names()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   beats  best  both  francisco  is  la  love  new  san  york\n",
      "0      0     0     0          1   0   0     1    0    1     0\n",
      "1      0     1     0          0   1   0     0    1    0     1\n",
      "2      1     0     1          0   0   1     0    0    0     0\n"
     ]
    }
   ],
   "source": [
    "# Create data frame\n",
    "df = pd.DataFrame(bag_of_words.toarray(), columns=feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.5844829010200651\n",
      "  (0, 2)\t0.5844829010200651\n",
      "  (0, 4)\t0.444514311537431\n",
      "  (0, 1)\t0.34520501686496574\n",
      "  (1, 1)\t0.3853716274664007\n",
      "  (1, 0)\t0.652490884512534\n",
      "  (1, 3)\t0.652490884512534\n",
      "  (2, 4)\t0.444514311537431\n",
      "  (2, 1)\t0.34520501686496574\n",
      "  (2, 6)\t0.5844829010200651\n",
      "  (2, 5)\t0.5844829010200651\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency – Inverse Document Frequency (TF – IDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.16685711\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [['data', 'science'], \n",
    "             ['science', 'data', 'analytics'], \n",
    "             ['machine', 'learning'], \n",
    "             ['Woodbury', 'computer', 'science'],\n",
    "             ['deep', 'learning']]\n",
    "\n",
    "# train the model on your corpus  \n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "print(model.wv.similarity('data', 'science'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041243795\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('Woodbury', 'science'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.8138672e-04 -3.8610934e-03  2.6190863e-03  4.8636068e-03\n",
      "  2.2238060e-03  4.3003969e-03 -3.8194559e-03 -2.8702852e-03\n",
      " -3.7463487e-03 -2.2866086e-03  1.5371896e-03  2.7930783e-03\n",
      "  1.2456301e-03 -2.4211605e-03  4.4885064e-03 -4.3097250e-03\n",
      "  1.4070435e-03  2.6501480e-03  1.5569822e-03 -2.5650887e-03\n",
      " -3.2691783e-03  2.5493090e-04 -4.7438140e-03  1.0247465e-03\n",
      "  8.3169230e-04 -3.2375345e-04 -4.2614643e-03  1.8137189e-05\n",
      "  3.2401725e-03  9.1164029e-04 -1.6952838e-03  1.0598367e-03\n",
      " -6.7888475e-05  3.2858809e-03  1.5157023e-03  1.7273537e-03\n",
      " -4.4889948e-03 -2.7818333e-03  4.6497090e-03  1.7127949e-03\n",
      " -1.0339462e-03 -3.5879831e-03 -1.5833150e-03  4.5878594e-03\n",
      "  2.2639406e-03 -3.2179763e-03  4.0261407e-04  4.9690693e-03\n",
      "  4.7700628e-04  2.0386970e-03 -3.9804252e-04 -1.8492838e-03\n",
      "  4.0697795e-03  2.5932947e-03 -2.4097192e-03  4.9152658e-03\n",
      "  1.5523387e-03 -3.0126399e-03  3.0247825e-03 -8.9566811e-04\n",
      " -3.0556088e-03  4.2012050e-03  3.7428113e-03 -6.3232391e-04\n",
      " -3.7477021e-03 -7.0102629e-04 -3.3463868e-03  4.6498929e-03\n",
      " -2.2164620e-03 -3.9615873e-03  4.2272736e-03  1.0254869e-03\n",
      " -2.6510700e-03 -1.4870355e-04 -1.9990620e-03  3.9642974e-04\n",
      "  4.3016332e-03  2.8538276e-03 -2.1972719e-03 -2.5945989e-04\n",
      " -1.5516028e-04 -2.1218432e-03  2.0019491e-03 -1.6565228e-04\n",
      " -3.6677269e-03 -3.3680790e-03 -3.1282157e-03 -4.4210018e-03\n",
      "  1.6111479e-03 -4.2502335e-03  2.6450392e-03 -1.8012810e-03\n",
      "  3.1820117e-03 -2.0088940e-03  4.7518718e-03 -1.2519425e-03\n",
      "  2.2455000e-03  4.4287677e-04 -2.8969445e-03 -1.7258206e-03]\n"
     ]
    }
   ],
   "source": [
    "print(model['Woodbury'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
