{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images.ndim =  3\n",
      "train_images.shape =  (60000, 28, 28)\n",
      "train_images.dtype =  uint8\n"
     ]
    }
   ],
   "source": [
    "# dimensions of the tensor train_images, stored in .ndim attribute\n",
    "print(\"train_images.ndim = \", train_images.ndim)\n",
    "\n",
    "# shape of the tensor train_images\n",
    "print(\"train_images.shape = \", train_images.shape)\n",
    "\n",
    "# data type of the tensor \n",
    "print(\"train_images.dtype = \", train_images.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# display one of the digits stored in the tensor \n",
    "digit = train_images[5]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# preprocess the data by reshaping it into the shape that the network expects\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "\n",
    "# scale the data so that all values are in the [0, 1] interval\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "# preprocess test data\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# categorically encode the labels\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 5.1208e-04 - acc: 1.0000\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 5.4006e-04 - acc: 0.9999\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 4.5630e-04 - acc: 0.9999\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 4.0797e-04 - acc: 1.0000\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 4.1433e-04 - acc: 0.9999\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 3.9904e-04 - acc: 0.9999\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 3.1495e-04 - acc: 1.0000\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 3.8959e-04 - acc: 0.9999\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 3.6948e-04 - acc: 1.0000\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 2.9347e-04 - acc: 1.0000\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 2.8857e-04 - acc: 1.0000\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 2.9879e-04 - acc: 1.0000\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 2.7490e-04 - acc: 1.0000\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 2.8115e-04 - acc: 1.0000\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 2.7484e-04 - acc: 1.0000\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 2.7220e-04 - acc: 1.0000\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 2.6921e-04 - acc: 1.0000\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 2.6883e-04 - acc: 1.0000\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 2.6886e-04 - acc: 1.0000\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 2.6884e-04 - acc: 1.0000\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 2.6877e-04 - acc: 1.0000\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6877e-04 - acc: 1.0000\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6877e-04 - acc: 1.0000\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 2.6876e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb36aa5dd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the network\n",
    "network.fit(train_images, train_labels, epochs=40, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 27us/step\n",
      "test_loss: 0.11862923611106153\n",
      "test_acc: 0.9833\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_loss:', test_loss)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 11s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Classifying IMDB movie reviews\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 1s 1us/step\n",
      "? this film was just brilliant casting ? ? story direction ? really ? the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same ? ? as myself so i loved the fact there was a real ? with this film the ? ? throughout the film were great it was just brilliant so much that i ? the film as soon as it was released for ? and would recommend it to everyone to watch and the ? ? was amazing really ? at the end it was so sad and you know what they say if you ? at a film it must have been good and this definitely was also ? to the two little ? that played the ? of ? and paul they were just brilliant children are often left out of the ? ? i think because the stars that play them all ? up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don't you think the whole story was so ? because it was true and was ? life after all that was ? with us all\n"
     ]
    }
   ],
   "source": [
    "# word_index is a dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "# We reverse it, mapping integer indices to words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# We decode the review; note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=1000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(1000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "24000/24000 [==============================] - 0s 8us/step - loss: 0.1482 - binary_accuracy: 0.9454 - val_loss: 0.4037 - val_binary_accuracy: 0.8470\n",
      "Epoch 2/20\n",
      "24000/24000 [==============================] - 0s 10us/step - loss: 0.1424 - binary_accuracy: 0.9501 - val_loss: 0.4034 - val_binary_accuracy: 0.8420\n",
      "Epoch 3/20\n",
      "24000/24000 [==============================] - 0s 8us/step - loss: 0.1396 - binary_accuracy: 0.9500 - val_loss: 0.4139 - val_binary_accuracy: 0.8400\n",
      "Epoch 4/20\n",
      "24000/24000 [==============================] - 0s 9us/step - loss: 0.1362 - binary_accuracy: 0.9516 - val_loss: 0.4166 - val_binary_accuracy: 0.8390\n",
      "Epoch 5/20\n",
      "24000/24000 [==============================] - 0s 9us/step - loss: 0.1310 - binary_accuracy: 0.9540 - val_loss: 0.4304 - val_binary_accuracy: 0.8370\n",
      "Epoch 6/20\n",
      "24000/24000 [==============================] - 0s 8us/step - loss: 0.1289 - binary_accuracy: 0.9557 - val_loss: 0.4546 - val_binary_accuracy: 0.8310\n",
      "Epoch 7/20\n",
      "24000/24000 [==============================] - 0s 8us/step - loss: 0.1253 - binary_accuracy: 0.9571 - val_loss: 0.4343 - val_binary_accuracy: 0.8360\n",
      "Epoch 8/20\n",
      "24000/24000 [==============================] - 0s 10us/step - loss: 0.1217 - binary_accuracy: 0.9575 - val_loss: 0.4585 - val_binary_accuracy: 0.8390\n",
      "Epoch 9/20\n",
      "24000/24000 [==============================] - 0s 10us/step - loss: 0.1178 - binary_accuracy: 0.9604 - val_loss: 0.4311 - val_binary_accuracy: 0.8340\n",
      "Epoch 10/20\n",
      "24000/24000 [==============================] - 0s 8us/step - loss: 0.1150 - binary_accuracy: 0.9630 - val_loss: 0.4826 - val_binary_accuracy: 0.8330\n",
      "Epoch 11/20\n",
      "24000/24000 [==============================] - 0s 10us/step - loss: 0.1115 - binary_accuracy: 0.9624 - val_loss: 0.4756 - val_binary_accuracy: 0.8320\n",
      "Epoch 12/20\n",
      "24000/24000 [==============================] - 0s 9us/step - loss: 0.1093 - binary_accuracy: 0.9635 - val_loss: 0.4745 - val_binary_accuracy: 0.8350\n",
      "Epoch 13/20\n",
      "24000/24000 [==============================] - 0s 8us/step - loss: 0.1042 - binary_accuracy: 0.9663 - val_loss: 0.4857 - val_binary_accuracy: 0.8310\n",
      "Epoch 14/20\n",
      "24000/24000 [==============================] - 0s 8us/step - loss: 0.1028 - binary_accuracy: 0.9663 - val_loss: 0.4867 - val_binary_accuracy: 0.8300\n",
      "Epoch 15/20\n",
      "24000/24000 [==============================] - 0s 8us/step - loss: 0.1004 - binary_accuracy: 0.9676 - val_loss: 0.5164 - val_binary_accuracy: 0.8310\n",
      "Epoch 16/20\n",
      "24000/24000 [==============================] - 0s 8us/step - loss: 0.0957 - binary_accuracy: 0.9695 - val_loss: 0.4981 - val_binary_accuracy: 0.8360\n",
      "Epoch 17/20\n",
      "24000/24000 [==============================] - 0s 8us/step - loss: 0.0956 - binary_accuracy: 0.9699 - val_loss: 0.5108 - val_binary_accuracy: 0.8330\n",
      "Epoch 18/20\n",
      "24000/24000 [==============================] - 0s 8us/step - loss: 0.0889 - binary_accuracy: 0.9739 - val_loss: 0.5163 - val_binary_accuracy: 0.8310\n",
      "Epoch 19/20\n",
      "24000/24000 [==============================] - 0s 9us/step - loss: 0.0868 - binary_accuracy: 0.9745 - val_loss: 0.5246 - val_binary_accuracy: 0.8310\n",
      "Epoch 20/20\n",
      "24000/24000 [==============================] - 0s 10us/step - loss: 0.0855 - binary_accuracy: 0.9742 - val_loss: 0.5393 - val_binary_accuracy: 0.8330\n"
     ]
    }
   ],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = y_train[:1000]\n",
    "partial_y_train = y_train[1000:]\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.11862923611106153\n",
      "test_acc: 0.9833\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "print('test_loss:', test_loss)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (2,) and (20,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-8c2f7b51c047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and validation accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2811\u001b[0m     return gca().plot(\n\u001b[1;32m   2812\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2813\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 231\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (2,) and (20,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['binary_accuracy']\n",
    "val_acc = history.history['val_binary_accuracy']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# what this image showed is the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30176914],\n",
       "       [0.9981862 ],\n",
       "       [0.6776088 ],\n",
       "       ...,\n",
       "       [0.11493229],\n",
       "       [0.03340629],\n",
       "       [0.6553505 ]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
