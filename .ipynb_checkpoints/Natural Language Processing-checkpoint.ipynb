{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Do you know how tokenization works?', \"It's actually quite interesting!\", \"Let's analyze a couple of sentences and figure it out.\"]\n",
      "\n",
      "Word tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n",
      "\n",
      "Word punct tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'Let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer\n",
    "\n",
    "# Define input text\n",
    "input_text = \"Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of sentences and figure it out.\" \n",
    "\n",
    "# Sentence tokenizer \n",
    "print(\"\\nSentence tokenizer:\")\n",
    "print(sent_tokenize(input_text))\n",
    "\n",
    "# Word tokenizer\n",
    "print(\"\\nWord tokenizer:\")\n",
    "print(word_tokenize(input_text))\n",
    "\n",
    "# WordPunct tokenizer\n",
    "print(\"\\nWord punct tokenizer:\")\n",
    "print(WordPunctTokenizer().tokenize(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n",
      "         writing           write            writ           write\n",
      "          calves            calv            calv            calv\n",
      "              be              be              be              be\n",
      "         branded           brand           brand           brand\n",
      "           horse            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possibly         possibl            poss         possibl\n",
      "       provision          provis          provid          provis\n",
      "        hospital          hospit          hospit          hospit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchy        scratchi\n",
      "            code            code             cod            code\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "input_words = ['writing', 'calves', 'be', 'branded', 'horse', 'randomize', \n",
    "        'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'code']\n",
    "\n",
    "# Create various stemmer objects\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Create a list of stemmer names for display\n",
    "stemmer_names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names), \n",
    "        '\\n', '='*68)\n",
    "\n",
    "# Stem each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, porter.stem(word), \n",
    "            lancaster.stem(word), snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      " ===========================================================================\n",
      "                 writing                 writing                   write\n",
      "                  calves                    calf                   calve\n",
      "                      be                      be                      be\n",
      "                 branded                 branded                   brand\n",
      "                   horse                   horse                   horse\n",
      "               randomize               randomize               randomize\n",
      "                possibly                possibly                possibly\n",
      "               provision               provision               provision\n",
      "                hospital                hospital                hospital\n",
      "                    kept                    kept                    keep\n",
      "                scratchy                scratchy                scratchy\n",
      "                    code                    code                    code\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "input_words = ['writing', 'calves', 'be', 'branded', 'horse', 'randomize', \n",
    "        'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'code']\n",
    "\n",
    "# Create lemmatizer object \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a list of lemmatizer names for display\n",
    "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names), \n",
    "        '\\n', '='*75)\n",
    "\n",
    "# Lemmatize each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, lemmatizer.lemmatize(word, pos='n'),\n",
    "           lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks = 18 \n",
      "\n",
      "Chunk 1 ==> The Fulton County Grand Jury said Friday an invest\n",
      "Chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city of\n",
      "Chunk 3 ==> . Construction bonds Meanwhile , it was learned th\n",
      "Chunk 4 ==> , anonymous midnight phone calls and veiled threat\n",
      "Chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451\n",
      "Chunk 6 ==> set it for public hearing on Feb. 22 . The proposa\n",
      "Chunk 7 ==> College . He has served as a border patrolman and \n",
      "Chunk 8 ==> of his staff were doing on the address involved co\n",
      "Chunk 9 ==> plan alone would boost the base to $5,000 a year a\n",
      "Chunk 10 ==> nursing homes In the area of `` community health s\n",
      "Chunk 11 ==> of its Angola policy prove harsh , there has been \n",
      "Chunk 12 ==> system which will prevent Laos from being used as \n",
      "Chunk 13 ==> reform in recipient nations . In Laos , the admini\n",
      "Chunk 14 ==> . He is not interested in being named a full-time \n",
      "Chunk 15 ==> said , `` to obtain the views of the general publi\n",
      "Chunk 16 ==> '' . Mr. Reama , far from really being retired , i\n",
      "Chunk 17 ==> making enforcement of minor offenses more effectiv\n",
      "Chunk 18 ==> to tell the people where he stands on the tax issu\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('brown')\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Split the input text into chunks, where\n",
    "# each chunk contains N words\n",
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    cur_chunk = []\n",
    "    count = 0\n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "\n",
    "    output.append(' '.join(cur_chunk))\n",
    "\n",
    "    return output \n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Read the first 12000 words from the Brown corpus\n",
    "    input_data = ' '.join(brown.words()[:12000])\n",
    "\n",
    "    # Define the number of words in each chunk \n",
    "    chunk_size = 700\n",
    "\n",
    "    chunks = chunker(input_data, chunk_size)\n",
    "    print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print('Chunk', i+1, '==>', chunk[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
